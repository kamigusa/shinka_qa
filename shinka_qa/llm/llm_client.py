"""
LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
OpenAI, Gemini, Anthropicã‚’ã‚µãƒãƒ¼ãƒˆ
è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è‡ªå‹•é¸æŠãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ã
"""

from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List, Tuple
import os
import sys


def safe_print(message: str):
    """
    Windowsã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚‚å®‰å…¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    çµµæ–‡å­—ãŒè¡¨ç¤ºã§ããªã„å ´åˆã¯ä»£æ›¿æ–‡å­—ã‚’ä½¿ç”¨
    """
    try:
        print(message)
    except UnicodeEncodeError:
        # çµµæ–‡å­—ã‚’ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆã«ç½®æ›
        replacements = {
            'ğŸ’°': '[$]',
            'âœ…': '[+]',
            'âš ï¸': '[!]',
            'âŒ': '[x]'
        }
        safe_message = message
        for emoji, replacement in replacements.items():
            safe_message = safe_message.replace(emoji, replacement)
        print(safe_message)


class LLMClient(ABC):
    """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ

        Args:
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0.0-1.0)
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        pass

    @abstractmethod
    def get_provider_name(self) -> str:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—"""
        pass

    @abstractmethod
    def get_model_name(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—"""
        pass

    @abstractmethod
    def get_cost_per_1m_tokens(self) -> Tuple[float, float]:
        """
        1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’å–å¾—

        Returns:
            (å…¥åŠ›ã‚³ã‚¹ãƒˆ, å‡ºåŠ›ã‚³ã‚¹ãƒˆ) ã®ã‚¿ãƒ—ãƒ« (USD)
        """
        pass


class OpenAIClient(LLMClient):
    """OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ–™é‡‘ (USD per 1M tokens)
    PRICING = {
        "gpt-5-nano": (0.50, 2.00),
        "gpt-4-turbo": (10.00, 30.00),
    }

    def __init__(self, api_key: str, model: str = "gpt-5-nano"):
        """
        Args:
            api_key: OpenAI APIã‚­ãƒ¼
            model: ãƒ¢ãƒ‡ãƒ«å (gpt-5-nano, gpt-4-turboç­‰)
        """
        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        self.model = model

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        try:
            params = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "max_completion_tokens": max_tokens
            }

            # gpt-5-nanoã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„
            if not ("gpt-5" in self.model.lower() and "nano" in self.model.lower()):
                params["temperature"] = temperature

            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content

        except Exception as e:
            print(f"OpenAI API error: {e}")
            return None

    def get_provider_name(self) -> str:
        return "OpenAI"

    def get_model_name(self) -> str:
        return self.model

    def get_cost_per_1m_tokens(self) -> Tuple[float, float]:
        """1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’å–å¾—"""
        return self.PRICING.get(self.model, (10.0, 30.0))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gpt-4-turbo


class GeminiClient(LLMClient):
    """Google Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ–™é‡‘ (USD per 1M tokens)
    PRICING = {
        "gemini-2.5-flash": (0.075, 0.30),
        "gemini-2.0-flash": (0.10, 0.40),
    }

    def __init__(self, api_key: str, model: str = "gemini-2.5-flash"):
        """
        Args:
            api_key: Google AI Studio APIã‚­ãƒ¼
            model: ãƒ¢ãƒ‡ãƒ«å (gemini-2.5-flash, gemini-2.0-flashç­‰)
        """
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        self.model_name = model
        self.model = genai.GenerativeModel(model)

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        try:
            # Gemini APIã§ã¯ã€system_instructionã‚’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æ™‚ã«è¨­å®šå¯èƒ½
            # ãŸã ã—ã€ã“ã“ã§ã¯å®Ÿè¡Œæ™‚ã«è¨­å®šã™ã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
            combined_prompt = f"{system_prompt}\n\n{user_prompt}"

            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
            }

            response = self.model.generate_content(
                combined_prompt,
                generation_config=generation_config
            )

            return response.text

        except Exception as e:
            print(f"Gemini API error: {e}")
            return None

    def get_provider_name(self) -> str:
        return "Google Gemini"

    def get_model_name(self) -> str:
        return self.model_name

    def get_cost_per_1m_tokens(self) -> Tuple[float, float]:
        """1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’å–å¾—"""
        return self.PRICING.get(self.model_name, (0.10, 0.40))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gemini-2.0-flash


class AnthropicClient(LLMClient):
    """Anthropic Claude APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ–™é‡‘ (USD per 1M tokens)
    PRICING = {
        "claude-4.5-haiku": (0.25, 1.25),
        "claude-3.5-sonnet": (3.00, 15.00),
    }

    def __init__(self, api_key: str, model: str = "claude-4.5-haiku"):
        """
        Args:
            api_key: Anthropic APIã‚­ãƒ¼
            model: ãƒ¢ãƒ‡ãƒ«å (claude-4.5-haiku, claude-3.5-sonnetç­‰)
        """
        from anthropic import Anthropic
        self.client = Anthropic(api_key=api_key)
        self.model = model

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )

            return response.content[0].text

        except Exception as e:
            print(f"Anthropic API error: {e}")
            return None

    def get_provider_name(self) -> str:
        return "Anthropic"

    def get_model_name(self) -> str:
        return self.model

    def get_cost_per_1m_tokens(self) -> Tuple[float, float]:
        """1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’å–å¾—"""
        return self.PRICING.get(self.model, (3.00, 15.00))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯claude-3.5-sonnet


class MultiProviderLLMClient(LLMClient):
    """
    è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç®¡ç†ã—ã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¡Œã†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """

    def __init__(self, clients: List[LLMClient]):
        """
        Args:
            clients: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        if not clients:
            raise ValueError("At least one client is required")

        # ã‚³ã‚¹ãƒˆã®å®‰ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå…¥åŠ›+å‡ºåŠ›ã®å¹³å‡ã‚³ã‚¹ãƒˆï¼‰
        self.clients = sorted(
            clients,
            key=lambda c: sum(c.get_cost_per_1m_tokens()) / 2
        )
        self.current_client_index = 0

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Optional[str]:
        """
        è¤‡æ•°ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦ã—ã¦ç”Ÿæˆ
        æœ€å®‰ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰é †ã«è©¦ã—ã€å¤±æ•—ã—ãŸã‚‰æ¬¡ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        last_error = None

        for i, client in enumerate(self.clients):
            try:
                if i == 0:
                    safe_print(f"ğŸ’° Using cheapest provider: {client.get_provider_name()} {client.get_model_name()}")
                else:
                    safe_print(f"âš ï¸  Fallback to: {client.get_provider_name()} {client.get_model_name()}")

                result = client.generate(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )

                if result is not None:
                    self.current_client_index = i
                    cost = client.get_cost_per_1m_tokens()
                    safe_print(f"âœ… Success! Cost: ${cost[0]:.3f}/${cost[1]:.3f} per 1M tokens (input/output)")
                    return result

            except Exception as e:
                last_error = e
                safe_print(f"âŒ {client.get_provider_name()} failed: {e}")
                continue

        safe_print(f"âŒ All providers failed. Last error: {last_error}")
        return None

    def get_provider_name(self) -> str:
        """ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—"""
        if self.current_client_index < len(self.clients):
            return self.clients[self.current_client_index].get_provider_name()
        return "MultiProvider"

    def get_model_name(self) -> str:
        """ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—"""
        if self.current_client_index < len(self.clients):
            return self.clients[self.current_client_index].get_model_name()
        return "Multiple"

    def get_cost_per_1m_tokens(self) -> Tuple[float, float]:
        """ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚³ã‚¹ãƒˆã‚’å–å¾—"""
        if self.current_client_index < len(self.clients):
            return self.clients[self.current_client_index].get_cost_per_1m_tokens()
        # å¹³å‡ã‚³ã‚¹ãƒˆã‚’è¿”ã™
        avg_input = sum(c.get_cost_per_1m_tokens()[0] for c in self.clients) / len(self.clients)
        avg_output = sum(c.get_cost_per_1m_tokens()[1] for c in self.clients) / len(self.clients)
        return (avg_input, avg_output)

    def get_available_providers(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆã‚³ã‚¹ãƒˆã®å®‰ã„é †ï¼‰"""
        return [
            f"{c.get_provider_name()} {c.get_model_name()} (${sum(c.get_cost_per_1m_tokens())/2:.3f}/1M avg)"
            for c in self.clients
        ]


def create_llm_client(
    provider: str,
    model: str,
    api_key: Optional[str] = None
) -> Optional[LLMClient]:
    """
    LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ

    Args:
        provider: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å ("openai", "gemini", "google", "anthropic")
        model: ãƒ¢ãƒ‡ãƒ«å
        api_key: APIã‚­ãƒ¼ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰

    Returns:
        LLMClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€å¤±æ•—æ™‚ã¯None
    """
    provider = provider.lower()

    # "google" ã‚’ "gemini" ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨ã—ã¦æ‰±ã†
    if provider == "google":
        provider = "gemini"

    # APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if api_key is None:
        if provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
        elif provider == "gemini":
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        elif provider == "anthropic":
            api_key = os.getenv("ANTHROPIC_API_KEY")

    if not api_key:
        print(f"Warning: No API key found for {provider}")
        return None

    try:
        if provider == "openai":
            return OpenAIClient(api_key=api_key, model=model)
        elif provider == "gemini":
            return GeminiClient(api_key=api_key, model=model)
        elif provider == "anthropic":
            return AnthropicClient(api_key=api_key, model=model)
        else:
            print(f"Unknown provider: {provider}")
            return None

    except Exception as e:
        print(f"Failed to create LLM client for {provider}: {e}")
        return None


def create_multi_provider_client(
    auto_detect: bool = True,
    providers: Optional[List[Dict[str, str]]] = None
) -> Optional[LLMClient]:
    """
    è¤‡æ•°ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’è‡ªå‹•æ¤œå‡ºã—ã€ã‚³ã‚¹ãƒˆã®å®‰ã„é †ã«ä½¿ç”¨

    Args:
        auto_detect: Trueã®å ´åˆã€ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•æ¤œå‡º
        providers: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã®ãƒªã‚¹ãƒˆ [{"provider": "gemini", "model": "gemini-2.5-flash"}, ...]

    Returns:
        MultiProviderLLMClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€å¤±æ•—æ™‚ã¯None
    """
    clients = []

    if auto_detect:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•æ¤œå‡º
        # Gemini (æœ€å®‰)
        gemini_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if gemini_key:
            try:
                client = GeminiClient(api_key=gemini_key, model="gemini-2.5-flash")
                clients.append(client)
                safe_print(f"âœ… Detected: Google Gemini (gemini-2.5-flash)")
            except Exception as e:
                safe_print(f"âš ï¸  Failed to initialize Gemini: {e}")

        # Anthropic (ã‚³ã‚¹ãƒ‘è‰¯)
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key:
            try:
                client = AnthropicClient(api_key=anthropic_key, model="claude-4.5-haiku")
                clients.append(client)
                safe_print(f"âœ… Detected: Anthropic Claude (claude-4.5-haiku)")
            except Exception as e:
                safe_print(f"âš ï¸  Failed to initialize Anthropic: {e}")

        # OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            try:
                client = OpenAIClient(api_key=openai_key, model="gpt-5-nano")
                clients.append(client)
                safe_print(f"âœ… Detected: OpenAI (gpt-5-nano)")
            except Exception as e:
                safe_print(f"âš ï¸  Failed to initialize OpenAI: {e}")

    elif providers:
        # æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        for config in providers:
            provider = config.get("provider", "").lower()
            model = config.get("model", "")
            client = create_llm_client(provider, model)
            if client:
                clients.append(client)

    if not clients:
        safe_print("âŒ No LLM providers available")
        return None

    return MultiProviderLLMClient(clients)
