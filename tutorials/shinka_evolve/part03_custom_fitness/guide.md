# Part 3: ã‚«ã‚¹ã‚¿ãƒ é©å¿œåº¦é–¢æ•° - ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚¬ã‚¤ãƒ‰

**æ‰€è¦æ™‚é–“**: 35åˆ†
**é›£æ˜“åº¦**: ä¸­ç´šã€œä¸Šç´š

---

## ğŸ¯ ã“ã®ãƒ‘ãƒ¼ãƒˆã§å­¦ã¶ã“ã¨

1. é©å¿œåº¦é–¢æ•°ã®è¨­è¨ˆåŸå‰‡
2. å¤šç›®çš„æœ€é©åŒ–
3. åˆ¶ç´„æ¡ä»¶ã®æ‰±ã„
4. ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®é©å¿œåº¦é–¢æ•°

---

## ğŸ¨ é©å¿œåº¦é–¢æ•°ã®è¨­è¨ˆåŸå‰‡

### åŸå‰‡1: æ˜ç¢ºãªç›®çš„

**æ‚ªã„ä¾‹**:
```python
def fitness(individual):
    # ä½•ã‚’æœ€é©åŒ–ã—ã¦ã„ã‚‹ï¼Ÿ
    return sum(individual) * np.mean(individual) / len(individual)
```

**è‰¯ã„ä¾‹**:
```python
def fitness(individual):
    """
    ç›®çš„: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç²¾åº¦ã‚’æœ€å¤§åŒ–
    individual: [learning_rate, num_layers, num_neurons]
    """
    model = create_model(individual)
    accuracy = train_and_evaluate(model)
    return accuracy
```

---

### åŸå‰‡2: é©åˆ‡ãªã‚¹ã‚±ãƒ¼ãƒ«

**å•é¡Œ**:
```python
def fitness(individual):
    obj1 = expensive_computation(individual)  # 0-1000
    obj2 = cheap_metric(individual)          # 0-1
    return obj1 + obj2  # obj2ãŒç„¡è¦–ã•ã‚Œã‚‹
```

**è§£æ±º**:
```python
def fitness(individual):
    obj1 = expensive_computation(individual) / 1000  # æ­£è¦åŒ–
    obj2 = cheap_metric(individual)
    return obj1 + obj2
```

---

### åŸå‰‡3: è¨ˆç®—åŠ¹ç‡

**é…ã„ä¾‹**:
```python
def fitness(individual):
    # ãƒ•ã‚¡ã‚¤ãƒ«I/Oï¼ˆéåŠ¹ç‡ï¼‰
    with open('data.txt', 'r') as f:
        data = f.read()
    return process(individual, data)
```

**é€Ÿã„ä¾‹**:
```python
# ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰
DATA = load_data()

def fitness(individual):
    return process(individual, DATA)
```

---

## ğŸ¯ å¤šç›®çš„æœ€é©åŒ–

### ã‚¹ã‚«ãƒ©ãƒ¼åŒ–ï¼ˆé‡ã¿ä»˜ãå’Œï¼‰

```python
def multi_objective_fitness(individual):
    # è¤‡æ•°ã®ç›®çš„
    accuracy = compute_accuracy(individual)     # æœ€å¤§åŒ–
    complexity = compute_complexity(individual) # æœ€å°åŒ–
    training_time = compute_time(individual)    # æœ€å°åŒ–

    # é‡ã¿ä»˜ãå’Œ
    w1, w2, w3 = 0.6, 0.2, 0.2

    # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    accuracy_normalized = accuracy  # 0-1
    complexity_normalized = complexity / 1000
    time_normalized = training_time / 100

    return (w1 * accuracy_normalized
            - w2 * complexity_normalized
            - w3 * time_normalized)
```

---

### ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–

```python
from shinka_evolve import NSGA2

def fitness_vector(individual):
    """è¤‡æ•°ã®ç›®çš„ã‚’è¿”ã™"""
    accuracy = compute_accuracy(individual)
    complexity = compute_complexity(individual)
    return [accuracy, -complexity]  # ä¸¡æ–¹æœ€å¤§åŒ–

# NSGA-IIï¼ˆéæ”¯é…ã‚½ãƒ¼ãƒˆéºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
evolution = NSGA2(
    fitness_function=fitness_vector,
    num_objectives=2,
    num_genes=10
)

pareto_front = evolution.evolve(num_generations=100)

# ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒƒãƒˆ
import matplotlib.pyplot as plt
objectives = [fitness_vector(ind) for ind in pareto_front]
acc, comp = zip(*objectives)
plt.scatter(acc, comp)
plt.xlabel('Accuracy')
plt.ylabel('Complexity')
plt.title('Pareto Front')
plt.show()
```

---

## ğŸš§ åˆ¶ç´„æ¡ä»¶ã®æ‰±ã„

### ãƒšãƒŠãƒ«ãƒ†ã‚£æ³•

```python
def fitness_with_constraint(individual):
    # ç›®çš„é–¢æ•°
    objective = compute_objective(individual)

    # åˆ¶ç´„æ¡ä»¶
    constraint_violation = 0

    # åˆ¶ç´„1: åˆè¨ˆãŒ100ä»¥ä¸‹
    if sum(individual) > 100:
        constraint_violation += (sum(individual) - 100) ** 2

    # åˆ¶ç´„2: å„è¦ç´ ãŒæ­£
    for x in individual:
        if x < 0:
            constraint_violation += abs(x) ** 2

    # ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°
    penalty = 1000

    return objective - penalty * constraint_violation
```

---

### ä¿®å¾©æ³•

```python
def repair(individual):
    """åˆ¶ç´„ã‚’æº€ãŸã™ã‚ˆã†ã«ä¿®æ­£"""
    # ç¯„å›²å¤–ã®å€¤ã‚’ä¿®æ­£
    individual = np.clip(individual, 0, 10)

    # åˆè¨ˆãŒ100ã‚’è¶…ãˆã‚‹å ´åˆã€æ­£è¦åŒ–
    if sum(individual) > 100:
        individual = individual * (100 / sum(individual))

    return individual

def fitness_with_repair(individual):
    # ä¿®å¾©
    individual = repair(individual)

    # è©•ä¾¡
    return compute_objective(individual)
```

---

## ğŸ§  ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®é©å¿œåº¦é–¢æ•°

### ä¾‹1: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ æ¢ç´¢ï¼ˆNASï¼‰

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
train_loader, val_loader = load_data()

def nas_fitness(individual):
    """
    individual: [num_layers, neurons_per_layer, dropout_rate, learning_rate]
    """
    num_layers = int(individual[0])
    neurons = int(individual[1])
    dropout = individual[2]
    lr = individual[3]

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
    layers = []
    for i in range(num_layers):
        layers.append(nn.Linear(neurons if i > 0 else 784, neurons))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout))
    layers.append(nn.Linear(neurons, 10))

    model = nn.Sequential(*layers)

    # è¨“ç·´
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(5):  # çŸ­æœŸè¨“ç·´
        for batch in train_loader:
            optimizer.zero_grad()
            loss = criterion(model(batch[0]), batch[1])
            loss.backward()
            optimizer.step()

    # è©•ä¾¡
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in val_loader:
            outputs = model(batch[0])
            _, predicted = torch.max(outputs, 1)
            total += batch[1].size(0)
            correct += (predicted == batch[1]).sum().item()

    accuracy = correct / total

    # ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆè¤‡é›‘æ€§ï¼‰
    num_params = sum(p.numel() for p in model.parameters())
    complexity_penalty = num_params / 1e6  # 100ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§1.0

    return accuracy - 0.1 * complexity_penalty

# é€²åŒ–å®Ÿè¡Œ
evolution = Evolution(
    fitness_function=nas_fitness,
    num_genes=4,
    bounds=[
        (1, 5),      # num_layers
        (32, 512),   # neurons
        (0, 0.5),    # dropout
        (1e-4, 1e-2) # learning_rate
    ],
    num_islands=4,
    population_per_island=20
)

best = evolution.evolve(num_generations=20)
print(f"Best architecture: {best}")
print(f"Fitness: {nas_fitness(best):.4f}")
```

---

### ä¾‹2: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å•é¡Œ

```python
# ã‚¿ã‚¹ã‚¯ã¨ãƒªã‚½ãƒ¼ã‚¹
tasks = [
    {'duration': 5, 'dependencies': []},
    {'duration': 3, 'dependencies': [0]},
    {'duration': 7, 'dependencies': [0]},
    {'duration': 2, 'dependencies': [1, 2]},
]

def scheduling_fitness(individual):
    """
    individual: ã‚¿ã‚¹ã‚¯ã®é–‹å§‹æ™‚åˆ»ã®ãƒªã‚¹ãƒˆ
    """
    # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
    violation = 0

    # ä¾å­˜é–¢ä¿‚ã®åˆ¶ç´„
    for i, task in enumerate(tasks):
        for dep in task['dependencies']:
            # ä¾å­˜ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¦ã‹ã‚‰é–‹å§‹
            if individual[i] < individual[dep] + tasks[dep]['duration']:
                violation += 1

    # ç›®çš„: å…¨ä½“ã®å®Œäº†æ™‚é–“ã‚’æœ€å°åŒ–
    completion_time = max(
        individual[i] + tasks[i]['duration']
        for i in range(len(tasks))
    )

    # ãƒšãƒŠãƒ«ãƒ†ã‚£ä»˜ãç›®çš„é–¢æ•°
    return -(completion_time + 1000 * violation)

evolution = Evolution(
    fitness_function=scheduling_fitness,
    num_genes=len(tasks),
    gene_type='real',
    bounds=[(0, 100)] * len(tasks)
)

best = evolution.evolve(num_generations=100)
print(f"Optimal schedule: {best}")
print(f"Completion time: {-scheduling_fitness(best)}")
```

---

### ä¾‹3: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–

```python
import numpy as np

# è³‡ç”£ãƒ‡ãƒ¼ã‚¿
returns = np.array([0.12, 0.18, 0.15, 0.10])  # æœŸå¾…åç›Šç‡
risks = np.array([0.05, 0.10, 0.08, 0.04])    # ãƒªã‚¹ã‚¯ï¼ˆæ¨™æº–åå·®ï¼‰
correlations = np.array([
    [1.0, 0.3, 0.2, 0.1],
    [0.3, 1.0, 0.4, 0.2],
    [0.2, 0.4, 1.0, 0.3],
    [0.1, 0.2, 0.3, 1.0]
])

def portfolio_fitness(individual):
    """
    individual: å„è³‡ç”£ã¸ã®æŠ•è³‡æ¯”ç‡ [w1, w2, w3, w4]
    """
    # æ­£è¦åŒ–ï¼ˆåˆè¨ˆã‚’1ã«ã™ã‚‹ï¼‰
    weights = individual / np.sum(individual)

    # æœŸå¾…åç›Šç‡
    expected_return = np.dot(weights, returns)

    # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ãƒªã‚¹ã‚¯
    portfolio_risk = np.sqrt(
        weights @ (correlations * np.outer(risks, risks)) @ weights
    )

    # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªï¼ˆãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³ï¼‰
    risk_free_rate = 0.02
    sharpe_ratio = (expected_return - risk_free_rate) / portfolio_risk

    return sharpe_ratio

evolution = Evolution(
    fitness_function=portfolio_fitness,
    num_genes=4,
    bounds=[(0, 1)] * 4,  # æŠ•è³‡æ¯”ç‡ 0-100%
    gene_type='real'
)

best = evolution.evolve(num_generations=100)
optimal_weights = best / np.sum(best)
print(f"Optimal allocation: {optimal_weights}")
print(f"Sharpe ratio: {portfolio_fitness(best):.4f}")
```

---

## ğŸ¯ å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’1: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

**ã‚¿ã‚¹ã‚¯**: scikit-learnã®RandomForestã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import cross_val_score

X, y = load_digits(return_X_y=True)

def fitness(individual):
    n_estimators, max_depth, min_samples_split = individual

    # æ•´æ•°å¤‰æ›
    n_estimators = int(n_estimators)
    max_depth = int(max_depth) if max_depth > 0 else None
    min_samples_split = int(min_samples_split)

    # ãƒ¢ãƒ‡ãƒ«
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        random_state=42
    )

    # äº¤å·®æ¤œè¨¼
    scores = cross_val_score(model, X, y, cv=3, n_jobs=-1)
    return scores.mean()

# ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰
evolution = Evolution(
    # TODO: è¨­å®š
)

best = evolution.evolve(num_generations=30)
```

---

### æ¼”ç¿’2: åˆ¶ç´„ä»˜ãæœ€é©åŒ–

**ã‚¿ã‚¹ã‚¯**: åˆ¶ç´„æ¡ä»¶ã‚’æº€ãŸã—ãªãŒã‚‰é–¢æ•°ã‚’æœ€é©åŒ–

```python
def fitness_constrained(individual):
    # ç›®çš„: f(x, y) = x^2 + y^2 ã‚’æœ€å°åŒ–
    objective = individual[0]**2 + individual[1]**2

    # åˆ¶ç´„: x + y >= 1
    constraint = individual[0] + individual[1] - 1

    if constraint < 0:
        # ãƒšãƒŠãƒ«ãƒ†ã‚£
        penalty = 1000 * constraint**2
        return -objective - penalty
    else:
        return -objective

# ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰
```

---

## ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] é©å¿œåº¦é–¢æ•°ã®è¨­è¨ˆåŸå‰‡ã‚’ç†è§£
- [ ] å¤šç›®çš„æœ€é©åŒ–ã‚’å®Ÿè£…
- [ ] åˆ¶ç´„æ¡ä»¶ã‚’æ‰±ã£ãŸ
- [ ] ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®é©å¿œåº¦é–¢æ•°ã‚’ä½œæˆ
- [ ] æ¼”ç¿’å•é¡Œã‚’è§£ã„ãŸ

**å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€Part 4ã«é€²ã¿ã¾ã—ã‚‡ã†ï¼**

---

**ä½œæˆæ—¥**: 2025-11-07
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
