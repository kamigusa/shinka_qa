# Part 4: å®Ÿå•é¡Œã¸ã®é©ç”¨ - ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚¬ã‚¤ãƒ‰

**æ‰€è¦æ™‚é–“**: 40åˆ†
**é›£æ˜“åº¦**: ä¸Šç´š

---

## ğŸ¯ ã“ã®ãƒ‘ãƒ¼ãƒˆã§å­¦ã¶ã“ã¨

1. å®Ÿä¸–ç•Œã®æœ€é©åŒ–å•é¡Œã¸ã®é©ç”¨
2. å¤§è¦æ¨¡å•é¡Œã®æ‰±ã„
3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
4. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

---

## ğŸ¤– ã‚±ãƒ¼ã‚¹1: æ©Ÿæ¢°å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

### å•é¡Œè¨­å®š

LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦ã€MNISTåˆ†é¡ã®ç²¾åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

```python
import lightgbm as lgb
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split, cross_val_score
from shinka_evolve import Evolution
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
X, y = load_digits(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

def lightgbm_fitness(individual):
    """
    individual: [learning_rate, num_leaves, max_depth,
                 min_child_samples, subsample, colsample_bytree]
    """
    learning_rate, num_leaves, max_depth, min_child_samples, subsample, colsample_bytree = individual

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›
    params = {
        'objective': 'multiclass',
        'num_class': 10,
        'metric': 'multi_logloss',
        'learning_rate': learning_rate,
        'num_leaves': int(num_leaves),
        'max_depth': int(max_depth),
        'min_child_samples': int(min_child_samples),
        'subsample': subsample,
        'colsample_bytree': colsample_bytree,
        'verbose': -1
    }

    # äº¤å·®æ¤œè¨¼
    try:
        dtrain = lgb.Dataset(X_train, label=y_train)
        cv_results = lgb.cv(
            params,
            dtrain,
            num_boost_round=100,
            nfold=3,
            early_stopping_rounds=10,
            verbose_eval=False
        )

        # æœ€è‰¯ã‚¹ã‚³ã‚¢
        best_score = max(cv_results['valid multi_logloss-mean'])
        return -best_score  # æœ€å°åŒ–ãªã®ã§è² ã«ã™ã‚‹

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä½ã„é©å¿œåº¦
        return -10.0

# é€²åŒ–è¨­å®š
evolution = Evolution(
    fitness_function=lightgbm_fitness,
    num_genes=6,
    gene_type='real',
    bounds=[
        (0.001, 0.3),   # learning_rate
        (10, 200),      # num_leaves
        (3, 15),        # max_depth
        (5, 100),       # min_child_samples
        (0.5, 1.0),     # subsample
        (0.5, 1.0)      # colsample_bytree
    ],
    num_islands=4,
    population_per_island=20,
    mutation_rate=0.1
)

# é€²åŒ–å®Ÿè¡Œ
print("Optimizing LightGBM hyperparameters...")
best = evolution.evolve(num_generations=50, verbose=True)

print(f"\nBest hyperparameters:")
print(f"  learning_rate: {best[0]:.4f}")
print(f"  num_leaves: {int(best[1])}")
print(f"  max_depth: {int(best[2])}")
print(f"  min_child_samples: {int(best[3])}")
print(f"  subsample: {best[4]:.4f}")
print(f"  colsample_bytree: {best[5]:.4f}")
print(f"\nBest fitness: {lightgbm_fitness(best):.6f}")

# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡
final_params = {
    'objective': 'multiclass',
    'num_class': 10,
    'learning_rate': best[0],
    'num_leaves': int(best[1]),
    'max_depth': int(best[2]),
    'min_child_samples': int(best[3]),
    'subsample': best[4],
    'colsample_bytree': best[5]
}

final_model = lgb.train(final_params, lgb.Dataset(X_train, label=y_train), num_boost_round=100)
test_pred = final_model.predict(X_test)
test_accuracy = (np.argmax(test_pred, axis=1) == y_test).mean()

print(f"\nTest accuracy: {test_accuracy:.4f}")
```

---

## ğŸ“Š ã‚±ãƒ¼ã‚¹2: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ æ¢ç´¢

### å•é¡Œè¨­å®š

CNNã®æ§‹é€ ï¼ˆå±¤æ•°ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ•°ãªã©ï¼‰ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

def build_cnn(individual):
    """
    individual: [num_conv_layers, num_filters, kernel_size, dropout, learning_rate]
    """
    num_conv_layers = int(individual[0])
    num_filters = int(individual[1])
    kernel_size = int(individual[2])
    dropout = individual[3]
    lr = individual[4]

    layers = []
    in_channels = 1

    # ç•³ã¿è¾¼ã¿å±¤
    for i in range(num_conv_layers):
        layers.append(nn.Conv2d(in_channels, num_filters, kernel_size))
        layers.append(nn.ReLU())
        layers.append(nn.MaxPool2d(2))
        in_channels = num_filters

    layers.append(nn.Flatten())

    # å…¨çµåˆå±¤ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    dummy_input = torch.zeros(1, 1, 28, 28)
    with torch.no_grad():
        for layer in layers:
            dummy_input = layer(dummy_input)
        fc_input_size = dummy_input.shape[1]

    layers.append(nn.Linear(fc_input_size, 128))
    layers.append(nn.ReLU())
    layers.append(nn.Dropout(dropout))
    layers.append(nn.Linear(128, 10))

    return nn.Sequential(*layers), lr

def cnn_fitness(individual):
    """CNNã‚’è¨“ç·´ã—ã¦è©•ä¾¡"""
    try:
        model, lr = build_cnn(individual)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)

        # è¨“ç·´ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ã‚’åˆ¶é™ï¼‰
        model.train()
        for epoch in range(3):  # çŸ­æœŸè¨“ç·´
            for batch_idx, (data, target) in enumerate(train_loader):
                if batch_idx > 50:  # ä¸€éƒ¨ã®ãƒãƒƒãƒã®ã¿
                    break
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

        # è©•ä¾¡
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)

        accuracy = correct / total

        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãƒšãƒŠãƒ«ãƒ†ã‚£
        num_params = sum(p.numel() for p in model.parameters())
        size_penalty = num_params / 1e6  # 100ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§1.0

        return accuracy - 0.05 * size_penalty

    except Exception as e:
        print(f"Error: {e}")
        return 0.0

# é€²åŒ–å®Ÿè¡Œ
evolution = Evolution(
    fitness_function=cnn_fitness,
    num_genes=5,
    bounds=[
        (1, 3),      # num_conv_layers
        (16, 64),    # num_filters
        (3, 5),      # kernel_size
        (0, 0.5),    # dropout
        (1e-4, 1e-2) # learning_rate
    ],
    num_islands=2,  # GPUä½¿ç”¨ã®å ´åˆã¯å°‘ãªã‚
    population_per_island=10
)

best = evolution.evolve(num_generations=10, verbose=True)
print(f"\nBest CNN architecture: {best}")
print(f"Best accuracy: {cnn_fitness(best):.4f}")
```

---

## ğŸ® ã‚±ãƒ¼ã‚¹3: ã‚²ãƒ¼ãƒ AIã®æœ€é©åŒ–

### å•é¡Œè¨­å®š

CartPoleç’°å¢ƒã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

```python
import gym
import numpy as np

def create_policy(weights):
    """é‡ã¿ã‹ã‚‰ãƒãƒªã‚·ãƒ¼ã‚’ä½œæˆ"""
    def policy(observation):
        # å˜ç´”ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        hidden = np.tanh(observation @ weights[:4].reshape(4, 8))
        output = hidden @ weights[4:].reshape(8, 2)
        return np.argmax(output)
    return policy

def evaluate_policy(weights, num_episodes=5):
    """ãƒãƒªã‚·ãƒ¼ã‚’è©•ä¾¡"""
    env = gym.make('CartPole-v1')
    total_reward = 0

    for _ in range(num_episodes):
        observation = env.reset()
        episode_reward = 0

        for _ in range(500):
            action = create_policy(weights)(observation)
            observation, reward, done, _ = env.step(action)
            episode_reward += reward

            if done:
                break

        total_reward += episode_reward

    env.close()
    return total_reward / num_episodes

def cartpole_fitness(individual):
    """é©å¿œåº¦é–¢æ•°"""
    return evaluate_policy(individual)

# é€²åŒ–å®Ÿè¡Œ
evolution = Evolution(
    fitness_function=cartpole_fitness,
    num_genes=4*8 + 8*2,  # é‡ã¿ã®æ•°
    gene_type='real',
    bounds=[(-1, 1)] * (4*8 + 8*2),
    num_islands=4,
    population_per_island=30
)

best = evolution.evolve(num_generations=50, verbose=True)
print(f"\nBest average reward: {cartpole_fitness(best):.2f}")

# ãƒ™ã‚¹ãƒˆãƒãƒªã‚·ãƒ¼ã‚’å¯è¦–åŒ–
env = gym.make('CartPole-v1', render_mode='human')
observation = env.reset()
for _ in range(500):
    env.render()
    action = create_policy(best)(observation)
    observation, reward, done, _ = env.step(action)
    if done:
        break
env.close()
```

---

## ğŸ† ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. é©å¿œåº¦é–¢æ•°ã®é«˜é€ŸåŒ–

```python
# ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_fitness(individual_tuple):
    individual = np.array(individual_tuple)
    return compute_fitness(individual)

def fitness(individual):
    return cached_fitness(tuple(individual))
```

---

### 2. æ—©æœŸåœæ­¢

```python
evolution = Evolution(
    fitness_function=fitness,
    early_stopping=True,
    patience=10,
    min_improvement=0.001
)
```

---

### 3. ãƒ­ã‚°è¨˜éŒ²

```python
import logging

logging.basicConfig(
    filename='evolution.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

def fitness_with_logging(individual):
    result = compute_fitness(individual)
    logging.info(f"Fitness: {result:.6f}, Individual: {individual}")
    return result
```

---

### 4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

```python
import pickle

def evolve_with_checkpoint(evolution, num_generations):
    for gen in range(num_generations):
        evolution.evolve(num_generations=1)

        # 10ä¸–ä»£ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if gen % 10 == 0:
            with open(f'checkpoint_gen{gen}.pkl', 'wb') as f:
                pickle.dump(evolution, f)

    return evolution.best_individual()
```

---

## ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] æ©Ÿæ¢°å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè£…
- [ ] ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ æ¢ç´¢ã‚’è©¦ã—ãŸ
- [ ] ã‚²ãƒ¼ãƒ AIã‚’æœ€é©åŒ–ã—ãŸ
- [ ] ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç†è§£
- [ ] è‡ªåˆ†ã®å•é¡Œã«é©ç”¨ã§ãã‚‹

**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼Shinka Evolveã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼**

---

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **è‡ªåˆ†ã®å•é¡Œã«é©ç”¨**
   - å®Ÿéš›ã®æ¥­å‹™ã§ä½¿ã£ã¦ã¿ã‚‹
   - çµæœã‚’æ¸¬å®šã™ã‚‹

2. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è²¢çŒ®**
   - æˆåŠŸäº‹ä¾‹ã‚’å…±æœ‰
   - æ–°ã—ã„é©å¿œåº¦é–¢æ•°ã‚’å…¬é–‹

3. **é«˜åº¦ãªãƒˆãƒ”ãƒƒã‚¯**
   - å…±é€²åŒ–
   - ãƒ‹ãƒƒãƒãƒ³ã‚°
   - é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶å¾¡

---

**ä½œæˆæ—¥**: 2025-11-07
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
